{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyson: Revolutionizing Computational Workload Distribution\n",
    "ðŸŒŸ The Intelligent Hardware Orchestrator\n",
    "\n",
    "In the complex world of high-performance computing, **Dyson** emerges as a game-changing solution, intelligently distributing computations across available hardware.\n",
    "\n",
    "ðŸš€ Analyze Once, Deploy Everywhere\n",
    "\n",
    "Imagine optimizing your code *automatically* across:\n",
    "* ðŸ’» CPUs\n",
    "* ðŸ–¥ï¸ GPUs\n",
    "* âš™ï¸ FPGAs\n",
    "...all without manually rewriting a single algorithm!\n",
    "\n",
    "ðŸ’¡ Key Benefits\n",
    "\n",
    "1. **â±ï¸ Time-Saving**: Eliminate manual hardware optimization tasks.\n",
    "2. **ðŸ› ï¸ Smart Distribution**: Ensure each computation runs on its ideal hardware.\n",
    "3. **ðŸ§  Simplified Development**: Focus on algorithms instead of hardware specifics.\n",
    "4. **ðŸ” Enhanced Performance**: Automatically leverage the strengths of each accelerator.\n",
    "5. **ðŸ”® Future-Proof**: Seamlessly integrate new hardware as it becomes available.\n",
    "\n",
    "## What is Dyson? ðŸ¤”\n",
    "\n",
    "Dyson is a sophisticated framework that intelligently divides your computational workload across different hardware accelerators. Rather than manually deciding which parts of your code should run where, Dyson analyzes your code and automatically distributes tasks to the most appropriate hardware. This ensures optimal performance by matching computational patterns with the hardware best suited to execute them efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       __                     \n",
      "  ____/ /_  ___________  ____ \n",
      " / __  / / / / ___/ __ \\/ __ \\\n",
      "/ /_/ / /_/ (__  ) /_/ / / / /\n",
      "\\__,_/\\__, /____/\\____/_/ /_/ \n",
      "     /____/                   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 15:08:16.142845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742549896.165772   35588 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742549896.172034   35588 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import dyson\n",
    "from dyson import DysonRouter\n",
    "import torch\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyson Router: Smart Workload Analysis and Distribution ðŸ§ \n",
    "\n",
    "The Dyson Router is the core component that analyzes your code to determine the optimal hardware for each computation:\n",
    "\n",
    "- ðŸ’» **CPU**: Best for sequential operations, control flow, and general-purpose tasks\n",
    "- ðŸ–¥ï¸ **GPU**: Optimized for parallel computations, matrix operations, and deep learning\n",
    "- âš™ï¸ **FPGA**: Ideal for specialized, custom-accelerated tasks requiring hardware-level optimization\n",
    "\n",
    "The Router examines the characteristics of your workloadâ€”operation types, data dependencies, parallelism opportunitiesâ€”and intelligently routes different components to the best available hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "router = DysonRouter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def video_compress(frame: jnp.ndarray, quantization_levels: int = 256) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    A toy example function to simulate video compression in JAX.\n",
    "    \n",
    "    Parameters:\n",
    "      - frame: A jnp.ndarray of shape (batch, channels, height, width) representing video frames.\n",
    "      - quantization_levels: Number of quantization levels (default is 256 for 8-bit quantization).\n",
    "    \n",
    "    The function performs:\n",
    "      1. Convolution with stride 2 to downsample (encode) the input frame.\n",
    "      2. Sigmoid activation to normalize the encoded features.\n",
    "      3. Quantization of the normalized features.\n",
    "    \n",
    "    Returns:\n",
    "      - A jnp.ndarray representing the compressed (encoded and quantized) video frame.\n",
    "    \"\"\"\n",
    "    # Get the number of channels from the input frame (assumes frame shape is NCHW)\n",
    "    channels = frame.shape[1]\n",
    "    \n",
    "    # Define a simple convolution kernel (averaging kernel) of shape (out_channels, in_channels, H, W)\n",
    "    # Here, both in_channels and out_channels equal 'channels'.\n",
    "    kernel = jnp.ones((channels, channels, 3, 3)) / 9.0\n",
    "\n",
    "    # Set convolution parameters:\n",
    "    # - strides: downsample spatial dimensions by 2.\n",
    "    # - padding: pad 1 pixel on each side to mimic PyTorch's padding=1.\n",
    "    # - dimension_numbers: specifying the layout for input ('NCHW'), kernel ('OIHW'), and output ('NCHW').\n",
    "    strides = (2, 2)\n",
    "    padding = [(1, 1), (1, 1)]  # For height and width dimensions\n",
    "\n",
    "    encoded = jax.lax.conv_general_dilated(\n",
    "        lhs=frame,\n",
    "        rhs=kernel,\n",
    "        window_strides=strides,\n",
    "        padding=padding,\n",
    "        dimension_numbers=('NCHW', 'OIHW', 'NCHW')\n",
    "    )\n",
    "    \n",
    "    # Apply sigmoid activation to normalize the values to [0, 1]\n",
    "    normalized = jax.nn.sigmoid(encoded)\n",
    "    \n",
    "    # Quantize the normalized values to simulate a lossy compression step.\n",
    "    quantized = jnp.round(normalized * (quantization_levels - 1)) / (quantization_levels - 1)\n",
    "    \n",
    "    return quantized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyson Router Parameters ðŸŽ›ï¸\n",
    "\n",
    "The `router.route_hardware()` function is the heart of Dyson's intelligent hardware selection system. Below is a detailed explanation of each parameter and how it influences hardware routing decisions.\n",
    "\n",
    "## Core Parameters\n",
    "\n",
    "| Parameter | Description | Example Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `tensor_ops` | The tensor operations to be routed | `tensor_ops_with_moderate_batch` |\n",
    "| `mode` | Optimization priority for hardware selection | `\"performance\"`, `\"energy-efficient\"`, `\"balanced\"` |\n",
    "| `judge` | Confidence level required (1-10) | `5` (moderate), `9` (high) |\n",
    "| `run_type` | Execution tracking and logging preferences | `\"log\"` |\n",
    "| `complexity` | Computational complexity of operations | `\"low\"`, `\"medium\"`, `\"high\"` |\n",
    "| `precision` | Required numerical precision | `\"low\"`, `\"normal\"`, `\"high\"` |\n",
    "| `multi_device` | Allow distribution across multiple devices | `True`, `False` |\n",
    "\n",
    "## Detailed Parameter Explanation\n",
    "\n",
    "### `tensor_ops` (Required)\n",
    "The computational operations you want to route to appropriate hardware. This can be a function, tensor operation set, or computational graph.\n",
    "\n",
    "\n",
    "### `mode` (Optional, Default: \"balanced\")\n",
    "Determines the primary optimization goal for hardware selection:\n",
    "\n",
    "- `\"performance\"`: Prioritize raw speed and throughput\n",
    "- `\"energy-efficient\"`: Minimize power consumption\n",
    "- `\"balanced\"`: Find an optimal balance between performance and power\n",
    "- `\"cost-effective\"`: Consider cloud/infrastructure costs\n",
    "\n",
    "\n",
    "### `judge` (Optional, Default: 5)\n",
    "Confidence threshold for routing decisions, ranging from 1 (low confidence required) to 10 (high confidence required):\n",
    "\n",
    "- `1-3`: Make rapid decisions with limited analysis\n",
    "- `4-6`: Perform moderate analysis before routing\n",
    "- `7-8`: Conduct thorough analysis (default)\n",
    "- `9-10`: Exhaustive analysis of all hardware options\n",
    "\n",
    "\n",
    "### `run_type` (Optional, Default: \"log\")\n",
    "Specifies the logging and monitoring behavior:\n",
    "\n",
    "- `\"log\"`: Basic logging of routing decisions\n",
    "- `\"debug\"`: Maximum information for troubleshooting\n",
    "\n",
    "\n",
    "### `complexity` (Optional, Default: \"medium\")\n",
    "Provides a hint about the computational complexity:\n",
    "\n",
    "- `\"low\"`: Simple operations (e.g., element-wise operations)\n",
    "- `\"medium\"`: Moderate complexity (e.g., matrix multiplications)\n",
    "- `\"high\"`: Complex operations (e.g., convolutions)\n",
    "\n",
    "\n",
    "### `precision` (Optional, Default: \"normal\")\n",
    "Required numerical precision for the computation:\n",
    "\n",
    "- `\"low\"`: Use lower precision (e.g., FP16, INT8)\n",
    "- `\"normal\"`: Standard precision (e.g., FP32)\n",
    "- `\"high\"`: High precision requirements (e.g., FP64)\n",
    "\n",
    "\n",
    "### `multi_device` (Optional, Default: False)\n",
    "Controls whether operations can be distributed across multiple devices:\n",
    "\n",
    "- `True`: Allow splitting operations across multiple hardware units\n",
    "- `False`: Constrain operations to a single hardware unit\n",
    "\n",
    "\n",
    "## Advanced Usage\n",
    "\n",
    "Combining multiple parameters allows for highly customized routing strategies:\n",
    "\n",
    "\n",
    "> ðŸ’¡ **Pro Tip**: For most use cases, you can rely on the default parameters and only specify those relevant to your specific requirements. Dyson's router is designed to make intelligent decisions with minimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the metadata we found {'batch_size': None, 'matrix_sizes': [(3, 3)], 'torch_fx_graph': None, 'jax_analysis': {'name': 'video_compress', 'signature': '(frame: jax.Array, quantization_levels: int = 256) -> jax.Array', 'module': '__main__', 'docstring': 'A toy example function to simulate video compression in JAX.\\n\\nParameters:\\n  - frame: A jnp.ndarray of shape (batch, channels, height, width) representing video frames.\\n  - quantization_levels: Number of quantization levels (default is 256 for 8-bit quantization).\\n\\nThe function performs:\\n  1. Convolution with stride 2 to downsample (encode) the input frame.\\n  2. Sigmoid activation to normalize the encoded features.\\n  3. Quantization of the normalized features.\\n\\nReturns:\\n  - A jnp.ndarray representing the compressed (encoded and quantized) video frame.', 'jax_operations': [], 'tensor_shapes': [(3, 3)], 'has_jit': False, 'has_grad': False, 'has_vmap': False, 'has_pmap': False, 'reduction_ops': [], 'transformation_ops': [], 'matrix_ops': [], 'code_metrics': {'loc': 45, 'complexity': 1}, 'computational_intensity': 'low'}, 'tf_graph': None, 'frameworks': ['cpp', 'jax', 'numpy'], 'ops_count': {'matrix_ops': 0, 'element_wise_ops': 0, 'reduction_ops': 0, 'convolution_ops': 0, 'transformation_ops': 0}, 'computational_intensity': 'low'}\n",
      "Model llama-7b response: {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '  Based on the analysis provided, I recommend using a compute-optimized CPU (c4cpu) for this workload. Here\\'s why:\\n\\n1. Matrix size thresholds: The matrices are small to medium in size (maximum dimension is 3), which falls under the \"small\" category. As a result, CPU processing should be sufficient for most operations.\\n2. Batch size thresholds: The batch size is small to medium (< 128), which also supports the use of CPU processing.\\n3. CPU vs GPU decision thresholds: The operations are primarily element-wise or have low parallelism, which favors CPU processing. Additionally, the total matrix elements are under 1M, and the batch sizes are small, which further supports using CPUs.\\n4. CPU-FIRST APPROACH: As the operations are primarily element-wise and have low parallelism, CPUs should be preferred when the operation count is low.\\n5. GPU JUSTIFICATION: The workload does not meet the critical thresholds for GPU usage, such as high data parallelism, large matrix dimensions, or specialized hardware benefits. Additionally, the performance gain from using a GPU would not offset the additional cost and energy usage.\\n\\nBased on these factors, I estimate that a compute-optimized CPU (c4cpu) would provide an estimated speedup of approximately 3-5x compared to a general-purpose CPU (n4cpu). The cost efficiency rating would be around 8-10, and the energy efficiency rating would be around 9-10. My confidence in this recommendation is high.\\n\\nTherefore, I recommend using a compute-optimized CPU (c4cpu) for this workload, given the small to medium matrix sizes, small batch sizes, and low parallelism of the operations.', 'role': 'assistant', 'tool_calls': []}}], 'created': 1742549935, 'id': 'cmpl-0642259ca0f74d6eab6a21ac6487a8ee', 'model': 'Llama-2-7b-chat', 'object': 'chat.completion', 'usage': {'completion_tokens': 395, 'prompt_tokens': 1898, 'total_tokens': 2293}}\n",
      "Model AI21 response: {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': ' {\\n    \"hardware_type\": \"c4cpu\",\\n    \"spec\": \"CPU-FIRST APPROACH, low computational intensity, small matrix size\",\\n    \"estimated_speedup\": \"1.5-2x\",\\n    \"cost_efficiency_rating\": \"9\",\\n    \"energy_efficiency_rating\": \"9\",\\n    \"confidence\": \"high\"\\n}', 'role': 'assistant', 'tool_calls': None}}], 'created': 1742549943, 'id': 'chat-7bde1014b2294ef59ee106755a11e90c', 'model': 'jamba-1.5-mini', 'object': 'chat.completion', 'usage': {'completion_tokens': 91, 'prompt_tokens': 1759, 'total_tokens': 1850}}\n",
      "Model Llama-3.1-8B response: {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Based on the task analysis and hardware configurations provided, I would recommend the following optimal hardware configuration for this workload:\\n\\n```json\\n{\\n    \"hardware_type\": \"c4acpu\",\\n    \"spec\": [\\n        \"Low computational intensity and small matrix dimensions make CPU processing a suitable choice.\",\\n        \"ARM-based Compute CPU provides excellent energy efficiency and cost-effectiveness.\",\\n        \"JAX\\'s performance on ARM CPUs is particularly good for smaller workloads.\"\\n    ],\\n    \"estimated_speedup\": \"1.2x-1.5x\",\\n    \"cost_efficiency_rating\": \"9\",\\n    \"energy_efficiency_rating\": \"10\",\\n    \"confidence\": \"high\"\\n}\\n```\\n\\nHere\\'s the reasoning behind this recommendation:\\n\\n1.  **Matrix Size and Computational Intensity**: The task involves small-scale matrix operations with a maximum dimension of 3, which is well within the CPU\\'s capabilities. The low computational intensity further supports the use of a CPU.\\n2.  **Energy Efficiency and Cost-Effectiveness**: The ARM-based Compute CPU (c4acpu) offers excellent energy efficiency and cost-effectiveness, making it an ideal choice for this workload.\\n3.  **JAX Performance**: JAX\\'s performance on ARM CPUs is particularly good for smaller workloads, which aligns with the characteristics of this task.\\n4.  **CPU-FIRST APPROACH**: The task meets several criteria for the CPU-FIRST APPROACH, including primarily element-wise operations, total matrix elements under 1M, small batch sizes, and sequential memory access patterns.\\n\\nWhile the NVIDIA T4 GPU (t4) is also a viable option, it would not provide a significant speedup for this workload due to the small matrix dimensions and low computational intensity. Additionally, the energy efficiency and cost-effectiveness of the c4acpu make it a more suitable choice.\\n\\nThe other hardware configurations, such as the NVIDIA L4 GPU (l4), NVIDIA V100 GPU (v100), Compute-optimized CPU (c4cpu), and General-purpose CPU (n4cpu), do not offer the same level of energy efficiency and cost-effectiveness as the c4acpu for this specific workload.', 'role': 'assistant', 'tool_calls': None}}], 'created': 1742549947, 'id': 'cmpl-ee205be3-3bc8-443f-85b6-9312a1a756cc', 'model': 'Meta-Llama-3.1-8B-Instruct', 'object': 'chat.completion', 'usage': {'completion_tokens': 439, 'prompt_tokens': 1555, 'total_tokens': 1994}}\n",
      "Error calling Llama-2-13b: 400, {\"error\":{\"code\":\"Bad Request\",\"message\":\"{\\\"object\\\":\\\"error\\\",\\\"message\\\":\\\"Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating\\\",\\\"type\\\":\\\"BadRequestError\\\",\\\"param\\\":null,\\\"code\\\":400}\",\"status\":400}}\n",
      "Judge decision: {'hardware_type': 'c4acpu', 'spec': \"Low computational intensity and small matrix dimensions make CPU processing a suitable choice. ARM-based Compute CPU provides excellent energy efficiency and cost-effectiveness. JAX's performance on ARM CPUs is particularly good for smaller workloads.\", 'confidence': 'high', 'cost_efficiency_score': 9, 'energy_efficiency_score': 10, 'performance_score': 8, 'alternative_option': 'c4cpu'}\n"
     ]
    }
   ],
   "source": [
    "hardware = router.route_hardware(\n",
    "        video_compress,\n",
    "        mode=\"energy-efficient\",\n",
    "        judge=5,\n",
    "        run_type=\"log\",\n",
    "        complexity=\"medium\",\n",
    "        precision=\"normal\",\n",
    "        multi_device=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The task involves general CPU operations with medium complexity and normal precision requirements. The ARM-based Compute CPU (c4acpu) is optimized for energy-efficient computing and supports NumPy computations efficiently. It offers excellent energy efficiency and cost-effectiveness, making it the most suitable choice for this task.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardware['spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c4acpu\n"
     ]
    }
   ],
   "source": [
    "print(hardware['hardware_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyson.run: Seamless Execution Across Hardware âš¡\n",
    "\n",
    "With `dyson.run`, you can execute your workload across multiple hardware accelerators seamlessly. Dyson handles all the complexity of splitting the computation, transferring data between devices, and recombining results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected framework: jax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#warmup the hardware\n",
    "def warmup():\n",
    "    a = jnp.array([1, 2, 3])\n",
    "    b = jnp.array([4, 5, 6])\n",
    "    return a+b\n",
    "\n",
    "func = dyson.run(warmup, hardware['hardware_type'])\n",
    "func()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_function = dyson.run(video_compress, hardware['hardware_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eager execution mode intelligently manages:\n",
    "- ðŸ”„ Partitioning your workload based on hardware affinity\n",
    "- ðŸ”Œ Cross-device communication and data synchronization\n",
    "- âš–ï¸ Load balancing to maximize resource utilization\n",
    "- ðŸ”§ Hardware-specific optimizations for each subcomponent\n",
    "\n",
    "Let's benchmark execution with and without Dyson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected framework: jax\n"
     ]
    }
   ],
   "source": [
    "dummy_frame = jnp.ones((1, 3, 256, 256))\n",
    "quantized_frame = compiled_function(dummy_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we seen that after warming up the instance it is taking the 4sec average for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.79215693, 0.882353  , 0.882353  , ..., 0.882353  ,\n",
       "          0.882353  , 0.882353  ],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         ...,\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124]],\n",
       "\n",
       "        [[0.79215693, 0.882353  , 0.882353  , ..., 0.882353  ,\n",
       "          0.882353  , 0.882353  ],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         ...,\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124]],\n",
       "\n",
       "        [[0.79215693, 0.882353  , 0.882353  , ..., 0.882353  ,\n",
       "          0.882353  , 0.882353  ],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         ...,\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124]]]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš¡ lets try one simple jax function that required energy efficient mode and require simple operation that can be handled by cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Comparison: GPU L4 vs. C4a CPU for Video Compression\n",
    "\n",
    "This notebook presents a cost and performance comparison of our video compression function when run on two different hardware configurations in our cloud:\n",
    "\n",
    "- **GPU L4**: A modern GPU instance optimized for parallel computation.\n",
    "- **C4a CPU**: A high-performance CPU instance selected for low-compute tasks after thorough analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Our video compression function, implemented in JAX, performs the following steps:\n",
    "1. **Convolution with stride 2** to downsample the video frame.\n",
    "2. **Sigmoid activation** to normalize the encoded features.\n",
    "3. **Quantization** to simulate a lossy compression step.\n",
    "\n",
    "\n",
    "\n",
    "## Benchmarking Code\n",
    "\n",
    "Below is the JAX implementation of the video compression function and the benchmarking code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's do a test with L4 gpus how much it cost you for a sinfgle function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected framework: jax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9], dtype=int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#warmup the hardware L4 gpu hardware\n",
    "def warmup():\n",
    "    a = jnp.array([1, 2, 3])\n",
    "    b = jnp.array([4, 5, 6])\n",
    "    return a+b\n",
    "\n",
    "func = dyson.run(warmup, 'l4')\n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_function = dyson.run(video_compress, 'l4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected framework: jax\n"
     ]
    }
   ],
   "source": [
    "dummy_frame = jnp.ones((1, 3, 256, 256))\n",
    "quantized_frame = compiled_function(dummy_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.79215693, 0.882353  , 0.882353  , ..., 0.882353  ,\n",
       "          0.882353  , 0.882353  ],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         ...,\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124]],\n",
       "\n",
       "        [[0.79215693, 0.882353  , 0.882353  , ..., 0.882353  ,\n",
       "          0.882353  , 0.882353  ],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         ...,\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124]],\n",
       "\n",
       "        [[0.79215693, 0.882353  , 0.882353  , ..., 0.882353  ,\n",
       "          0.882353  , 0.882353  ],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         ...,\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124],\n",
       "         [0.882353  , 0.95294124, 0.95294124, ..., 0.95294124,\n",
       "          0.95294124, 0.95294124]]]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Summary and Cost Analysis\n",
    "\n",
    "We benchmarked our video compression function on two hardware configurations in our cloud:\n",
    "\n",
    "- **GPU L4**\n",
    "- **C4a CPU**\n",
    "\n",
    "## Benchmark Results\n",
    "\n",
    "| Hardware    | Execution Time | Hourly Cost                    |\n",
    "|-------------|----------------|--------------------------------|\n",
    "| **GPU L4**  | 4 seconds      | \\$1.00 (Standard 8 vCPU)       |\n",
    "| **C4a CPU** | 6 seconds      | \\$0.21                         |\n",
    "\n",
    "### Observations:\n",
    "- **Performance:**  \n",
    "  - GPU L4 completes the task in **4 seconds**, which is about **33% faster** than the C4a CPU's **6 seconds**.\n",
    "  \n",
    "- **Cost Efficiency:**  \n",
    "  - Despite being slower, the C4a CPU is much more cost-effective at **\\$0.21 per hour** compared to **\\$1.00 per hour** for the standard 8 vCPU instance.\n",
    "\n",
    "## Cost per Execution Analysis\n",
    "\n",
    "The cost per execution is calculated using the formula:\n",
    "\n",
    "\\[\n",
    "\\text{Cost per Execution} = \\left(\\frac{\\text{Execution Time (seconds)}}{3600}\\right) \\times \\text{Hourly Cost}\n",
    "\\]\n",
    "\n",
    "The following table summarizes our results:\n",
    "\n",
    "| Hardware    | Execution Time (sec) | Hourly Cost (USD) | Cost per Execution (USD)                                   |\n",
    "|-------------|----------------------|-------------------|------------------------------------------------------------|\n",
    "| **GPU L4**  | 4                    | \\$1.00            | \\$0.00111         |\n",
    "| **C4a CPU** | 6                    | \\$0.21            | \\$0.00035         |\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **GPU L4**:  \n",
    "  - Faster execution at **4 seconds** per run.  \n",
    "  - Cost per execution is approximately **\\$0.00111**.\n",
    "  \n",
    "- **C4a CPU**:  \n",
    "  - Slower execution at **6 seconds** per run.  \n",
    "  - More cost-effective with a cost per execution of approximately **\\$0.00035**.\n",
    "\n",
    "This analysis demonstrates that while the GPU L4 offers lower latency, the C4a CPU is significantly more cost-efficient per execution.\n",
    "\n",
    "\n",
    "**Recommendation:**  \n",
    "If the application can tolerate slightly higher latency, the **C4a CPU** configuration offers significant cost savings. However, if speed is critical, the **GPU L4** configuration is preferable despite its higher cost.\n",
    "\n",
    "This detailed breakdown helps demonstrate that while our cloud solution can deliver exceptional performance with GPU L4, the C4a CPU option provides a very attractive cost-performance balance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Pro Tip**: While Dyson automatically distributes your workload, you can provide hints or constraints when you have specific requirements for certain operations.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Dyson provides:\n",
    "1. Automatic analysis of your code to determine optimal hardware\n",
    "2. Intelligent distribution of workload components across different accelerators\n",
    "3. Seamless execution that handles all the complexity of cross-device computation\n",
    "4. Potential performance improvements through hardware specialization\n",
    "\n",
    "Now you're ready to maximize computational efficiency by letting Dyson intelligently distribute your workload across the right hardware! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's try some more complex example to deep dive into the dyson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h264_compression(frame1, frame2, block_size=8):\n",
    "    \"\"\"\n",
    "    Performs H.264-like compression including DCT, quantization, motion estimation, and reconstruction.\n",
    "\n",
    "    Parameters:\n",
    "        frame1 (jax.numpy.ndarray): The current video frame (grayscale, 2D array).\n",
    "        frame2 (jax.numpy.ndarray): The reference frame for motion estimation (grayscale, 2D array).\n",
    "        block_size (int, optional): Size of the blocks used for DCT and motion estimation (default: 8).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - reconstructed_frame (jax.numpy.ndarray): The frame reconstructed after compression.\n",
    "            - motion_vectors (jax.numpy.ndarray): Motion vectors indicating displacement for each block.\n",
    "    \n",
    "    Steps:\n",
    "        1. Apply 2D Discrete Cosine Transform (DCT) to each block in frame1.\n",
    "        2. Quantize the DCT coefficients using a standard quantization matrix.\n",
    "        3. Dequantize and apply inverse DCT to reconstruct the frame.\n",
    "        4. Perform motion estimation using block matching between frame1 and frame2.\n",
    "    \"\"\"\n",
    "    N = block_size\n",
    "    DCT_matrix = jnp.array([[jnp.cos((2 * i + 1) * j * jnp.pi / (2 * N)) for j in range(N)] for i in range(N)])\n",
    "    DCT_matrix = DCT_matrix * jnp.sqrt(2 / N)\n",
    "    DCT_matrix = DCT_matrix.at[0].set(DCT_matrix[0] / jnp.sqrt(2))\n",
    "\n",
    "    def dct_2d(block):\n",
    "        return jnp.dot(DCT_matrix, jnp.dot(block, DCT_matrix.T))\n",
    "\n",
    "    def idct_2d(coeff):\n",
    "        return jnp.dot(DCT_matrix.T, jnp.dot(coeff, DCT_matrix))\n",
    "\n",
    "    def quantize(block, Q):\n",
    "        return jnp.round(block / Q)\n",
    "\n",
    "    def dequantize(block, Q):\n",
    "        return block * Q\n",
    "\n",
    "    Q_matrix = jnp.array(\n",
    "        [[16, 11, 10, 16, 24, 40, 51, 61],\n",
    "         [12, 12, 14, 19, 26, 58, 60, 55],\n",
    "         [14, 13, 16, 24, 40, 57, 69, 56],\n",
    "         [14, 17, 22, 29, 51, 87, 80, 62],\n",
    "         [18, 22, 37, 56, 68, 109, 103, 77],\n",
    "         [24, 35, 55, 64, 81, 104, 113, 92],\n",
    "         [49, 64, 78, 87, 103, 121, 120, 101],\n",
    "         [72, 92, 95, 98, 112, 100, 103, 99]])\n",
    "\n",
    "    height, width = frame1.shape\n",
    "    motion_vectors = jnp.zeros((height // block_size, width // block_size, 2))\n",
    "    reconstructed_frame = jnp.zeros_like(frame1)\n",
    "\n",
    "    for i in range(0, height, block_size):\n",
    "        for j in range(0, width, block_size):\n",
    "            block = frame1[i:i+block_size, j:j+block_size]\n",
    "            dct_coeff = dct_2d(block)\n",
    "            quantized = quantize(dct_coeff, Q_matrix)\n",
    "            dequantized = dequantize(quantized, Q_matrix)\n",
    "            reconstructed_block = idct_2d(dequantized)\n",
    "            reconstructed_frame = reconstructed_frame.at[i:i+block_size, j:j+block_size].set(reconstructed_block)\n",
    "            \n",
    "            best_match = jnp.array([0, 0])\n",
    "            min_error = jnp.inf\n",
    "            for dx in range(-4, 5):\n",
    "                for dy in range(-4, 5):\n",
    "                    ref_x, ref_y = i + dx, j + dy\n",
    "                    is_valid_x = (ref_x >= 0) & (ref_x < height - block_size)\n",
    "                    is_valid_y = (ref_y >= 0) & (ref_y < width - block_size)\n",
    "                    if is_valid_x & is_valid_y:\n",
    "                        candidate = frame2[ref_x:ref_x+block_size, ref_y:ref_y+block_size]\n",
    "                        error = jnp.sum(jnp.abs(block - candidate))\n",
    "                        condition = error < min_error\n",
    "                        min_error = jnp.where(condition, error, min_error)\n",
    "                        best_match = jnp.where(condition, jnp.array([dx, dy]), best_match)\n",
    "\n",
    "            motion_vectors = motion_vectors.at[i//block_size, j//block_size].set(best_match)\n",
    "\n",
    "    return reconstructed_frame, motion_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's route this function to dyson router to find the best hardware for cost-effective mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array\n",
      "set\n",
      "array\n",
      "zeros\n",
      "zeros_like\n",
      "sqrt\n",
      "dot\n",
      "dot\n",
      "round\n",
      "sqrt\n",
      "dot\n",
      "dot\n",
      "set\n",
      "array\n",
      "set\n",
      "cos\n",
      "sum\n",
      "where\n",
      "where\n",
      "abs\n",
      "array\n",
      "Model llama-7b recommendation:   Based on the task characteristics and available hardware configurations, I recommend using a NVIDIA T4 GPU for the given task.\n",
      "\n",
      "Here's my reasoning:\n",
      "\n",
      "1. Matrix sizes and computational intensity: The task requires medium complexity, which falls within the T4's performance sweet spot. The T4 has 16 GB of GPU memory, which should be sufficient for most inference tasks.\n",
      "2. Batch processing requirements: The task does not require batch processing, so the T4's performance in small-scale inference tasks is sufficient.\n",
      "3. Precision needs (normal): The task requires normal precision, which the T4 can handle with ease.\n",
      "4. Cost-efficiency requirements (cost-effective): The T4 offers high cost efficiency, making it an ideal choice for cost-effective mode.\n",
      "5. Framework-specific optimizations: The T4 has excellent support for PyTorch, TensorFlow, and JAX, making it an excellent choice for these frameworks.\n",
      "6. Memory requirements: The T4 has 16 GB of GPU memory, which should be sufficient for most inference tasks.\n",
      "7. Energy efficiency needs: The T4 offers good energy efficiency, which is important for cost-effective mode.\n",
      "\n",
      "Here's the JSON format recommendation:\n",
      "\n",
      "{\n",
      "\"hardware_type\": \"t4\",\n",
      "\"spec\": \"T4 offers excellent performance for small to medium-scale inference tasks, making it an ideal choice for this task. Its cost efficiency and support for PyTorch, TensorFlow, and JAX make it a great choice for cost-effective mode. Additionally, its good energy efficiency makes it suitable for energy-efficient computing.\"\n",
      "}\n",
      "Model AI21 recommendation:  {\n",
      "    \"hardware_type\": \"c4cpu\",\n",
      "    \"spec\": \"Given the task's medium complexity, normal precision requirements, and general CPU operations with NumPy, the compute-optimized CPU (c4cpu) is the most suitable option. This configuration offers high cost-efficiency and parallel processing capabilities, making it ideal for CPU-intensive computations and batch processing. The c4cpu also provides efficient numerical computations, aligning well with the task's operational context.\"\n",
      "}\n",
      "Model phi-3.5-MOE recommendation:  {\n",
      "    \"hardware_type\": \"c4cpu\",\n",
      "    \"spec\": \"The task involves general CPU operations with numpy and does not require high precision or large matrix operations. Given the medium complexity level and cost-effective optimization mode, a compute-optimized CPU (c4cpu) is the most suitable hardware configuration for this task. It offers good parallel processing, efficient numerical computations, and cost efficiency.\"\n",
      "}\n",
      "Model Llama-3.1-8B recommendation: Based on the task characteristics and compute requirements, I recommend the following hardware configuration:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"hardware_type\": \"t4\",\n",
      "    \"spec\": \"The task requires general CPU operations with medium complexity and normal precision requirements. The cost-effective mode is specified, making the NVIDIA T4 GPU a suitable choice. Although the T4 has limited memory (16GB), it is sufficient for small-scale operations. Additionally, the T4's cost efficiency and good FP16 performance make it an optimal choice for this task.\"\n",
      "}\n",
      "```\n",
      "\n",
      "Justification:\n",
      "\n",
      "1. **Matrix sizes and computational intensity**: The task does not specify large matrix sizes, which makes the T4's 16GB memory sufficient. The computational intensity is also not extremely high, given the medium complexity level.\n",
      "\n",
      "2. **Batch processing requirements**: The task specifies \"None\" batch size, which means the T4's capability for small to medium batch inference is not a limiting factor.\n",
      "\n",
      "3. **Precision needs**: Normal precision requirements are specified, which is within the capabilities of the T4.\n",
      "\n",
      "4. **Cost-efficiency requirements**: The cost-effective mode is specified, making the T4's high cost efficiency a major advantage.\n",
      "\n",
      "5. **Framework-specific optimizations**: The task involves NumPy operations, which is well-suited for the T4's capabilities.\n",
      "\n",
      "6. **Memory requirements**: Although the T4 has limited memory, it is sufficient for small-scale operations.\n",
      "\n",
      "7. **Energy efficiency needs**: The T4 is not the most energy-efficient option, but it is a cost-effective choice that balances energy efficiency with performance.\n",
      "\n",
      "In summary, the NVIDIA T4 GPU is a suitable choice for this task due to its cost efficiency, good FP16 performance, and ability to handle small-scale operations.\n"
     ]
    }
   ],
   "source": [
    "hardware = router.route_hardware(\n",
    "        h264_compression,\n",
    "        mode=\"cost-effective\",\n",
    "        judge=5,\n",
    "        run_type=\"log\",\n",
    "        complexity=\"medium\",\n",
    "        precision=\"normal\",\n",
    "        multi_device=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the task's medium complexity, normal precision requirements, and general CPU operations with NumPy, the compute-optimized CPU (c4cpu) is the most suitable option. This configuration offers high cost-efficiency and parallel processing capabilities, making it ideal for CPU-intensive computations and batch processing. The c4cpu also provides efficient numerical computations, aligning well with the task's operational context.\n"
     ]
    }
   ],
   "source": [
    "print(hardware['spec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c4cpu\n"
     ]
    }
   ],
   "source": [
    "print(hardware['hardware_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for c4cpu...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n"
     ]
    }
   ],
   "source": [
    "compiled_function = dyson.run(h264_compression, target_device=hardware['hardware_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for c4cpu...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n",
      "[INFO] Detected framework: jax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#warmup the hardware c4cpu gpu hardware\n",
    "def warmup():\n",
    "    a = jnp.array([1, 2, 3])\n",
    "    b = jnp.array([4, 5, 6])\n",
    "    return a+b\n",
    "\n",
    "func = dyson.run(warmup, 'c4cpu')\n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected framework: jax\n",
      "total time taken by c4cpu : 8.121525287628174\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tn = time.time()\n",
    "import numpy as np\n",
    "frame1 = jnp.array(np.random.randint(0, 255, (32, 32)), dtype=np.float32)\n",
    "frame2 = jnp.array(np.random.randint(0, 255, (32, 32)), dtype=np.float32)\n",
    "reconstructed_frame, motion_vectors = compiled_function(frame1, frame2)\n",
    "print(f\"total time taken by c4cpu : {time.time()-tn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[521.8932    -10.550179  -99.42299   ...  22.26274    52.26022\n",
      "  -75.997444 ]\n",
      " [-48.505497    2.3863568 -29.551645  ...  59.150383  127.56043\n",
      "  108.17874  ]\n",
      " [286.67725    30.665321  -40.67891   ... -41.05825    -4.488559\n",
      "  151.4014   ]\n",
      " ...\n",
      " [143.53181    97.671936  -68.15375   ...  -9.166077  129.90804\n",
      "   27.815    ]\n",
      " [-74.29934    56.14898   -13.276386  ... -20.28918   116.7041\n",
      "  117.718124 ]\n",
      " [150.80908   -47.68759   104.093575  ...  66.43942    67.01185\n",
      "  -26.234825 ]] [[[ 0.  4.]\n",
      "  [ 2. -1.]\n",
      "  [ 2. -1.]\n",
      "  [ 2. -1.]]\n",
      "\n",
      " [[ 0.  3.]\n",
      "  [-2.  2.]\n",
      "  [-4.  3.]\n",
      "  [ 4. -4.]]\n",
      "\n",
      " [[-3.  0.]\n",
      "  [-2.  1.]\n",
      "  [-2. -1.]\n",
      "  [ 2. -3.]]\n",
      "\n",
      " [[-3.  2.]\n",
      "  [-3. -2.]\n",
      "  [-2.  2.]\n",
      "  [-2. -2.]]]\n"
     ]
    }
   ],
   "source": [
    "print(reconstructed_frame, motion_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's compare this with a l4 gpu and see banchmarks on time and cost ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for l4...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n"
     ]
    }
   ],
   "source": [
    "compiled_function_l4 = dyson.run(h264_compression, target_device='l4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for l4...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n",
      "[INFO] Detected framework: jax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#warmup the hardware l4 gpu hardware\n",
    "def warmup():\n",
    "    a = jnp.array([1, 2, 3])\n",
    "    b = jnp.array([4, 5, 6])\n",
    "    return a+b\n",
    "\n",
    "func = dyson.run(warmup, 'l4')\n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected framework: jax\n",
      "total time taken by l4 : 17.957362174987793\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tn = time.time()\n",
    "import numpy as np\n",
    "frame1 = jnp.array(np.random.randint(0, 255, (32, 32)), dtype=np.float32)\n",
    "frame2 = jnp.array(np.random.randint(0, 255, (32, 32)), dtype=np.float32)\n",
    "reconstructed_frame, motion_vectors = compiled_function_l4(frame1, frame2)\n",
    "print(f\"total time taken by l4 : {time.time()-tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Summary and Cost Analysis\n",
    "\n",
    "We benchmarked our video compression function on two hardware configurations in our cloud:\n",
    "\n",
    "- **GPU L4**\n",
    "- **C4 CPU**\n",
    "\n",
    "## Benchmark Results\n",
    "\n",
    "| Hardware    | Execution Time | Hourly Cost                    |\n",
    "|-------------|----------------|--------------------------------|\n",
    "| **GPU L4**  | 17.95 seconds  | \\$1.00 (Standard 8 vCPU)       |\n",
    "| **C4 CPU**  | 8.12 seconds   | \\$0.21                         |\n",
    "\n",
    "### Observations:\n",
    "- **Performance:**  \n",
    "  - C4 CPU completes the task in **8.12 seconds**, which is **54.8% faster** than the GPU L4's **17.95 seconds**.\n",
    "  \n",
    "- **Cost Efficiency:**  \n",
    "  - The C4 CPU is significantly more cost-effective at **\\$0.21 per hour** compared to **\\$1.00 per hour** for the standard 8 vCPU L4 GPU instance.\n",
    "\n",
    "## Cost per Execution Analysis\n",
    "\n",
    "The cost per execution is calculated using the formula:\n",
    "\n",
    "```\n",
    "Cost per Execution = (Execution Time (seconds) / 3600) Ã— Hourly Cost\n",
    "```\n",
    "\n",
    "The following table summarizes our results:\n",
    "\n",
    "| Hardware    | Execution Time (sec) | Hourly Cost (USD) | Cost per Execution (USD) | \n",
    "|-------------|----------------------|-------------------|--------------------------|\n",
    "| **GPU L4**  | 17.95                | \\$1.00            | \\$0.00499                |\n",
    "| **C4 CPU**  | 8.12                 | \\$0.21            | \\$0.00047                |\n",
    "\n",
    "### Cost Savings Analysis\n",
    "\n",
    "- **Cost Savings per Execution:** \\$0.00452 (\\$0.00499 - \\$0.00047)\n",
    "- **Percentage Cost Savings:** 90.6%\n",
    "- **Cost Ratio:** C4 CPU is 10.6x more cost-efficient per execution than GPU L4\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **GPU L4**:  \n",
    "  - Slower execution at **17.95 seconds** per run.  \n",
    "  - Cost per execution is approximately **\\$0.00499**.\n",
    "  \n",
    "- **C4 CPU**:  \n",
    "  - Faster execution at **8.12 seconds** per run.  \n",
    "  - Significantly more cost-effective with a cost per execution of approximately **\\$0.00047**.\n",
    "\n",
    "This analysis demonstrates that the C4 CPU offers both lower latency and dramatically lower costs per execution compared to the GPU L4 configuration for this particular video compression workload.\n",
    "\n",
    "**Recommendation:**  \n",
    "The **C4 CPU** configuration is clearly superior for this workload, offering both better performance (54.8% faster) and substantial cost savings (90.6% lower cost per execution). We recommend standardizing on C4 CPU instances for this video compression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try with an example that read a image from cloud and convert into gray and return the array. this will show how dyson can handle the dynamic code ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Benchmark Analysis: Video Frame Processing with Remote Data Access\n",
    "\n",
    "This benchmark demonstrates Dyson's ability to process remotely accessed data (images) across different hardware configurations:\n",
    "\n",
    "- **GPU L4**\n",
    "- **C4 CPU**\n",
    "\n",
    "## Function Details\n",
    "\n",
    "```python\n",
    "def process_video_frame():\n",
    "    \"\"\"\n",
    "    Reads a video frame from a remote URL, processes it (converts to grayscale),\n",
    "    and returns the processed frame.\n",
    "    \"\"\"\n",
    "    # Remote data access - fetching image from GitHub\n",
    "    image_url = \"https://avatars.githubusercontent.com/u/170319640?s=200&v=4\"\n",
    "    resp = urlopen(image_url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Convert to grayscale (basic transformation)\n",
    "    gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return gray_frame\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from urllib.request import urlopen\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frame():\n",
    "    \"\"\"\n",
    "    Reads a video frame from an image file, processes it (converts to grayscale),\n",
    "    and writes the processed frame to another file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): Path to the input image file.\n",
    "    - output_path (str): Path to save the processed image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the image (simulating a video frame)\n",
    "    image_url = \"https://avatars.githubusercontent.com/u/170319640?s=200&v=4\"\n",
    "    resp = urlopen(image_url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR) # The image object\n",
    "\n",
    "\n",
    "    # Convert to grayscale (basic transformation)\n",
    "    gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Save the processed frame\n",
    "    return gray_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asarray\n",
      "imdecode\n",
      "cvtColor\n",
      "read\n",
      "Model llama-7b recommendation:   Based on the provided information, my recommendation for the most suitable hardware configuration for the given task is:\n",
      "\n",
      "{\n",
      "\"hardware_type\": \"t4\",\n",
      "\"spec\": \"T4 GPU offers the best balance of computational intensity, batch processing requirements, and cost-efficiency for normal precision needs. Its 16GB GPU memory is sufficient for small to medium-sized matrices and batches, while its high FP16 performance makes it suitable for general CPU operations in PyTorch, TensorFlow, and JAX. Additionally, its high cost efficiency makes it a good choice for cost-effective mode. \"\n",
      "}\n",
      "\n",
      "My reasoning is based on the following factors:\n",
      "\n",
      "1. Matrix sizes and computational intensity: The task requires normal precision, which means that the matrix sizes and computational intensity are not extremely large. T4 GPU offers a good balance of performance and cost-efficiency for this type of workload.\n",
      "2. Batch processing requirements: The task does not require batch processing, but it does require general CPU operations in PyTorch, TensorFlow, and JAX. T4 GPU's high FP16 performance makes it suitable for these operations.\n",
      "3. Precision needs: The task requires normal precision, which means that T4 GPU's 16GB GPU memory is sufficient for small to medium-sized matrices and batches.\n",
      "4. Cost-efficiency requirements: T4 GPU offers high cost efficiency, making it a good choice for cost-effective mode.\n",
      "5. Framework-specific optimizations: T4 GPU provides good support for PyTorch, TensorFlow, and JAX, and its high FP16 performance makes it suitable for general CPU operations in these frameworks.\n",
      "6. Memory requirements: T4 GPU's 16GB GPU memory is sufficient for small to medium-sized matrices and batches.\n",
      "7. Energy efficiency needs: T4 GPU offers high energy efficiency, which is important for cost-effective mode.\n",
      "\n",
      "In summary, T4 GPU offers the best balance of computational intensity, batch processing requirements, and cost-efficiency for normal precision needs, making it the most suitable hardware configuration for the given task.\n",
      "Model AI21 recommendation:  Based on the provided task analysis and the available hardware configurations, here is the recommended hardware configuration:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"hardware_type\": \"c4cpu\",\n",
      "    \"spec\": \"Given the task's medium complexity, normal precision requirements, and the need for cost-effective general CPU operations (numpy), the Compute-optimized CPU (c4cpu) is the most suitable option. It offers high cost efficiency and parallel processing capabilities, making it ideal for CPU-intensive computations and batch processing.\"\n",
      "}\n",
      "```\n",
      "\n",
      "### Justification:\n",
      "\n",
      "1. **Complexity and Precision**: The task complexity is medium, and normal precision is required. The c4cpu provides efficient numerical computations suitable for this level of precision.\n",
      "2. **Operation Types**: The task involves general CPU operations (numpy), which are efficiently handled by the c4cpu.\n",
      "3. **Cost Efficiency**: The c4cpu is highly cost-efficient, aligning with the optimization mode requirement.\n",
      "4. **Memory Requirements**: While the c4cpu does not specify GPU memory, it is designed for CPU-intensive tasks which aligns with the task's needs.\n",
      "5. **Energy Efficiency**: The c4cpu is not explicitly stated to be energy-efficient, but its cost efficiency implies a balance between performance and power consumption.\n",
      "6. **Framework-specific Optimizations**: The c4cpu supports all frameworks for basic operations, making it versatile for various computational needs.\n",
      "\n",
      "The c4cpu strikes a balance between cost efficiency, performance, and suitability for the task's requirements, making it the optimal choice.\n",
      "Model phi-3.5-MOE recommendation:  {1 task: <|\n",
      "        {1 CPU: <based on: < Please\n",
      "        \"c104 CPU\n",
      "       \n",
      "        - <task\n",
      "       \n",
      "       \n",
      "\n",
      "        - GPU\n",
      "Model Llama-3.1-8B recommendation: Based on the provided task characteristics and decision criteria, I recommend the following hardware configuration:\n",
      "\n",
      "{\n",
      "    \"hardware_type\": \"t4\",\n",
      "    \"spec\": \"Given the medium complexity level, normal precision requirements, and general CPU operations, the T4 GPU is a cost-effective option that provides good FP16 performance. Although it has limited memory (16GB), it is sufficient for the task, and the T4 is optimal for small to medium batch inference and computer vision tasks, which aligns with the task's characteristics. Additionally, the T4 is the best choice for PyTorch in computer vision inference and small batch training, which is a key factor in this case.\"\n",
      "}\n",
      "\n",
      "The deciding factors for this recommendation are:\n",
      "\n",
      "1. **Matrix sizes and computational intensity**: Since the matrix dimensions are not provided, we assume a medium-scale operation. The T4 is suitable for small to medium-scale operations.\n",
      "2. **Batch processing requirements**: Since the batch size is \"None,\" we don't have to prioritize batch processing. However, the T4 is still a good option for small batch inference.\n",
      "3. **Precision needs (normal)**: The T4 provides good FP16 performance, which meets the normal precision requirements.\n",
      "4. **Cost-efficiency requirements (cost-effective)**: The T4 is a cost-effective option, making it a good choice for the cost-effective mode.\n",
      "5. **Framework-specific optimizations**: The T4 is the best choice for PyTorch in computer vision inference and small batch training.\n",
      "6. **Memory requirements**: Although the T4 has limited memory (16GB), it should be sufficient for the task.\n",
      "7. **Energy efficiency needs**: The T4 is not the most energy-efficient option, but it's a good compromise between cost and performance.\n"
     ]
    }
   ],
   "source": [
    "hardware = router.route_hardware(\n",
    "        process_video_frame,\n",
    "        mode=\"cost-effective\",\n",
    "        judge=5,\n",
    "        run_type=\"log\",\n",
    "        complexity=\"medium\",\n",
    "        precision=\"normal\",\n",
    "        multi_device=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the task's medium complexity, normal precision requirements, and the need for cost-effective general CPU operations (numpy), the Compute-optimized CPU (c4cpu) is the most suitable option. It offers high cost efficiency and parallel processing capabilities, making it ideal for CPU-intensive computations and batch processing.\n"
     ]
    }
   ],
   "source": [
    "print(hardware['spec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c4cpu\n"
     ]
    }
   ],
   "source": [
    "print(hardware['hardware_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for c4cpu...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n"
     ]
    }
   ],
   "source": [
    "compiled_simple_function_c4 = dyson.run(process_video_frame, target_device=hardware['hardware_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected framework: numpy\n",
      "[[2 2 2 ... 1 1 5]\n",
      " [3 1 1 ... 5 3 1]\n",
      " [7 1 0 ... 7 6 2]\n",
      " ...\n",
      " [3 3 5 ... 3 2 1]\n",
      " [2 2 2 ... 2 0 1]\n",
      " [7 2 2 ... 1 0 3]]\n",
      "total time taken by c4 cpu: 1.3370757102966309\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tn = time.time()\n",
    "print(compiled_simple_function_c4())\n",
    "print(f\"total time taken by c4 cpu: {time.time()-tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's compare this simple function with a gpu instance like l4 how much time it takes it costs you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for l4...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n"
     ]
    }
   ],
   "source": [
    "compiled_simple_function_l4 = dyson.run(process_video_frame, target_device='l4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected framework: numpy\n",
      "[[2 2 2 ... 1 1 5]\n",
      " [3 1 1 ... 5 3 1]\n",
      " [7 1 0 ... 7 6 2]\n",
      " ...\n",
      " [3 3 5 ... 3 2 1]\n",
      " [2 2 2 ... 2 0 1]\n",
      " [7 2 2 ... 1 0 3]]\n",
      "total time taken by l4 gpu: 1.5854814052581787\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tn = time.time()\n",
    "print(compiled_simple_function_l4())\n",
    "print(f\"total time taken by l4 gpu: {time.time()-tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Results\n",
    "\n",
    "| Hardware    | Execution Time | Hourly Cost                    |\n",
    "|-------------|----------------|--------------------------------|\n",
    "| **GPU L4**  | 1.58 seconds   | \\$1.00 (Standard 8 vCPU)       |\n",
    "| **C4 CPU**  | 1.33 seconds   | \\$0.21                         |\n",
    "\n",
    "### Observations:\n",
    "- **Performance:**  \n",
    "  - C4 CPU completes the task in **1.33 seconds**, which is **15.8% faster** than the GPU L4's **1.58 seconds**.\n",
    "  - The function successfully demonstrates remote data access capabilities, fetching an image from GitHub and processing it.\n",
    "  \n",
    "- **Cost Efficiency:**  \n",
    "  - The C4 CPU is significantly more cost-effective at **\\$0.21 per hour** compared to **\\$1.00 per hour** for the L4 GPU instance.\n",
    "\n",
    "## Cost per Execution Analysis\n",
    "\n",
    "| Hardware    | Execution Time (sec) | Hourly Cost (USD) | Cost per Execution (USD) | \n",
    "|-------------|----------------------|-------------------|--------------------------|\n",
    "| **GPU L4**  | 1.58                 | \\$1.00            | \\$0.00044                |\n",
    "| **C4 CPU**  | 1.33                 | \\$0.21            | \\$0.00008                |\n",
    "\n",
    "### Cost Savings Analysis\n",
    "\n",
    "- **Cost Savings per Execution:** \\$0.00036 (\\$0.00044 - \\$0.00008)\n",
    "- **Percentage Cost Savings:** 81.8%\n",
    "- **Cost Ratio:** C4 CPU is 5.5x more cost-efficient per execution than GPU L4\n",
    "\n",
    "## Key Highlights\n",
    "\n",
    "1. **Remote Data Access:** The function successfully demonstrates Dyson's ability to access and process remote data (images) from external URLs.\n",
    "\n",
    "2. **Performance Advantage:** Despite L4 GPUs typically being optimized for image processing, the C4 CPU performs better for this specific workload involving remote data access and basic image processing.\n",
    "\n",
    "3. **Cost Efficiency:** C4 CPU offers substantial cost savings (81.8% lower cost per execution) compared to L4 GPU.\n",
    "\n",
    "## Recommendation\n",
    "\n",
    "For workloads involving remote data access and basic image processing:\n",
    "- **C4 CPU** is recommended for both performance and cost efficiency\n",
    "- This configuration is ideal for production deployments where both speed and cost are important considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This benchmark evaluates a data preprocessing ETL (Extract, Transform, Load) pipeline that processes large datasets requiring significant memory resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "def titanic_etl(data):\n",
    "    \"\"\"\n",
    "    ETL function that:\n",
    "    1. Downloads Titanic dataset from a URL\n",
    "    2. Preprocesses the data (cleaning, feature engineering)\n",
    "    \n",
    "    Args:\n",
    "        csv_url (str): URL to the Titanic CSV file\n",
    "        bucket_name (str): GCP bucket name to store the processed data\n",
    "    Returns:\n",
    "        pandas.DataFrame: The processed DataFrame\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, \n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger('titanic_etl')\n",
    "    \n",
    "    # Step 1: Extract - Download the CSV file\n",
    "    \n",
    "    # Step 2: Transform - Preprocess the Titanic dataset\n",
    "    logger.info(\"Preprocessing Titanic dataset\")\n",
    "    try:\n",
    "        # Make a copy to avoid modifying the original\n",
    "        processed_df = data.copy()\n",
    "        \n",
    "        # 2.1 Handle missing values\n",
    "        # Fill missing age with median\n",
    "        processed_df['Age'] = processed_df['Age'].fillna(processed_df['Age'].median())\n",
    "        \n",
    "        # Fill missing embarked with most common value\n",
    "        most_common_embarked = processed_df['Embarked'].mode()[0]\n",
    "        processed_df['Embarked'] = processed_df['Embarked'].fillna(most_common_embarked)\n",
    "        \n",
    "        # Fill missing cabin with 'Unknown'\n",
    "        processed_df['Cabin'] = processed_df['Cabin'].fillna('Unknown')\n",
    "        \n",
    "        # 2.2 Feature engineering\n",
    "        # Extract title from name\n",
    "        processed_df['Title'] = processed_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        \n",
    "        # Group rare titles\n",
    "        rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "        processed_df.loc[processed_df['Title'].isin(rare_titles), 'Title'] = 'Rare'\n",
    "        processed_df.loc[processed_df['Title'] == 'Mlle', 'Title'] = 'Miss'\n",
    "        processed_df.loc[processed_df['Title'] == 'Ms', 'Title'] = 'Miss'\n",
    "        processed_df.loc[processed_df['Title'] == 'Mme', 'Title'] = 'Mrs'\n",
    "        \n",
    "        # Create family size feature\n",
    "        processed_df['FamilySize'] = processed_df['SibSp'] + processed_df['Parch'] + 1\n",
    "        \n",
    "        # Create is_alone feature\n",
    "        processed_df['IsAlone'] = (processed_df['FamilySize'] == 1).astype(int)\n",
    "        \n",
    "        # Extract deck from cabin\n",
    "        processed_df['Deck'] = processed_df['Cabin'].str[0]\n",
    "        processed_df['Deck'] = processed_df['Deck'].fillna('U')\n",
    "        \n",
    "        # 2.3 Categorical encoding\n",
    "        # Convert categorical features to numeric\n",
    "        processed_df['Sex'] = processed_df['Sex'].map({'male': 0, 'female': 1})\n",
    "        \n",
    "        # One-hot encode embarked\n",
    "        embarked_dummies = pd.get_dummies(processed_df['Embarked'], prefix='Embarked')\n",
    "        processed_df = pd.concat([processed_df, embarked_dummies], axis=1)\n",
    "        \n",
    "        # One-hot encode title\n",
    "        title_dummies = pd.get_dummies(processed_df['Title'], prefix='Title')\n",
    "        processed_df = pd.concat([processed_df, title_dummies], axis=1)\n",
    "        \n",
    "        # One-hot encode deck\n",
    "        deck_dummies = pd.get_dummies(processed_df['Deck'], prefix='Deck')\n",
    "        processed_df = pd.concat([processed_df, deck_dummies], axis=1)\n",
    "        \n",
    "        # 2.4 Drop unnecessary columns\n",
    "        columns_to_drop = ['Name', 'Ticket', 'Cabin', 'Embarked', 'Title', 'Deck']\n",
    "        processed_df = processed_df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        logger.info(f\"Preprocessing complete. Final dataframe shape: {processed_df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during preprocessing: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basicConfig\n",
      "getLogger\n",
      "info\n",
      "copy\n",
      "fillna\n",
      "fillna\n",
      "fillna\n",
      "extract\n",
      "astype\n",
      "fillna\n",
      "map\n",
      "get_dummies\n",
      "concat\n",
      "get_dummies\n",
      "concat\n",
      "get_dummies\n",
      "concat\n",
      "drop\n",
      "info\n",
      "median\n",
      "mode\n",
      "error\n",
      "isin\n",
      "Model llama-7b recommendation:   Based on the provided task characteristics and available hardware configurations, I recommend using a NVIDIA V100 GPU (v100) for the given task.\n",
      "\n",
      "Here's my reasoning:\n",
      "\n",
      "1. Matrix sizes and computational intensity: The task requires medium complexity, which falls within the V100's strength. The V100 has 32GB of GPU memory, which should be sufficient for most medium-sized matrices.\n",
      "2. Batch processing requirements: The task does not have batch processing requirements, so the V100's high Tensor Cores performance is not a factor in this case.\n",
      "3. Precision needs (normal): The task requires normal precision, which the V100 can handle with ease.\n",
      "4. Cost-efficiency requirements (energy-efficient): The V100 has a medium cost efficiency, which is acceptable for the given task.\n",
      "5. Framework-specific optimizations: The V100 has excellent FP16 performance, which can further improve the performance of PyTorch, TensorFlow, and JAX.\n",
      "6. Memory requirements: The V100 has 32GB of GPU memory, which should be sufficient for most medium-sized matrices.\n",
      "7. Energy efficiency needs: The V100 has excellent energy efficiency, which is important for the given task.\n",
      "\n",
      "In summary, the V100 offers the best balance of performance, cost-efficiency, and energy efficiency for the given task.\n",
      "\n",
      "Here's the JSON recommendation:\n",
      "{\n",
      "\"hardware_type\": \"v100\",\n",
      "\"spec\": \"The V100 offers the best balance of performance, cost-efficiency, and energy efficiency for the given task, with its 32GB of GPU memory and excellent FP16 performance. Its Tensor Cores also provide a boost for PyTorch, TensorFlow, and JAX. Despite being a more expensive option, the V100's high performance and energy efficiency make it the best choice for the given task.\"\n",
      "}\n",
      "Model AI21 recommendation:  ```json\n",
      "{\n",
      "\"hardware_type\": \"c4cpu\",\n",
      "\"spec\": \"The task primarily involves general CPU operations (NumPy) and requires cost-efficiency with a focus on energy-efficiency. The c4cpu configuration offers high cost-efficiency and excellent performance for NumPy operations, making it the most suitable choice for this task.\"\n",
      "}\n",
      "```\n",
      "Model phi-3.5-MOE recommendation:  {\n",
      "    \"hardware_type\": \"c4acpu\",\n",
      "    \"spec\": \"The task has medium complexity with normal precision requirements and no specific batch size or matrix dimensions, indicating a need for general CPU operations without the necessity for high-performance GPUs. The focus on energy-efficient mode and the use of NumPy suggest that the ARM-based Compute CPU (c4acpu) is the most suitable choice. It offers excellent energy efficiency, is optimized for ARM workloads, and is capable of handling CPU-bound operations efficiently.\"\n",
      "}\n",
      "Model Llama-3.1-8B recommendation: Based on the task characteristics and compute requirements, I recommend the following hardware configuration:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"hardware_type\": \"c4acpu\",\n",
      "    \"spec\": \"Given the task complexity level is medium, precision requirements are normal, and the need for energy-efficient mode, the ARM-based Compute CPU (c4acpu) is the most suitable choice. It provides excellent energy efficiency, high cost efficiency, and is optimized for ARM-optimized numerical computations, making it ideal for this task.\"\n",
      "}\n",
      "```\n",
      "\n",
      "Justification:\n",
      "\n",
      "1. **Matrix sizes and computational intensity**: The task complexity level is medium, indicating a moderate level of computational intensity. While the matrix dimensions are not specified, the task involves 20 identical general CPU operations, suggesting a need for a balance between performance and energy efficiency. The c4acpu's optimized architecture for ARM-optimized workloads makes it suitable for this task.\n",
      "\n",
      "2. **Batch processing requirements**: The batch size is None, indicating no batch processing requirements. This is not a significant factor in the decision, as the c4acpu is still a good choice for CPU-bound operations.\n",
      "\n",
      "3. **Precision needs (normal)**: Normal precision requirements are met with the c4acpu's ability to handle general CPU operations.\n",
      "\n",
      "4. **Cost-efficiency requirements (energy-efficient)**: The c4acpu is designed for energy-efficient computing and has excellent energy efficiency, making it a cost-effective choice.\n",
      "\n",
      "5. **Framework-specific optimizations**: While the task uses NumPy, the c4acpu is optimized for ARM-optimized numerical computations, making it a suitable choice.\n",
      "\n",
      "6. **Memory requirements**: The c4acpu's memory is not specified, but it is likely sufficient for this task, given the moderate computational intensity.\n",
      "\n",
      "7. **Energy efficiency needs**: The c4acpu's excellent energy efficiency makes it an ideal choice for this task.\n",
      "\n",
      "In summary, the c4acpu's balance of performance, cost efficiency, and energy efficiency makes it the most suitable hardware configuration for this task.\n"
     ]
    }
   ],
   "source": [
    "hardware = router.route_hardware(\n",
    "        titanic_etl,\n",
    "        mode=\"energy-efficient\",\n",
    "        judge=5,\n",
    "        run_type=\"log\",\n",
    "        complexity=\"medium\",\n",
    "        precision=\"normal\",\n",
    "        multi_device=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The task has medium complexity with normal precision requirements and no specific batch size or matrix dimensions, indicating a need for general CPU operations without the necessity for high-performance GPUs. The focus on energy-efficient mode and the use of NumPy suggest that the ARM-based Compute CPU (c4acpu) is the most suitable choice. It offers excellent energy efficiency, is optimized for ARM workloads, and is capable of handling CPU-bound operations efficiently.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardware['spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c4acpu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardware['hardware_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train.csv downloaded to gcp_train.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/your_key.json\"\n",
    "from google.cloud import storage\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    \n",
    "    print(f\"File {source_blob_name} downloaded to {destination_file_name}.\")\n",
    "\n",
    "download_blob(\"dysontest\", \"train.csv\", \"gcp_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CSV URL (Titanic dataset)\n",
    "df = pd.read_csv(\"gcp_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for c4acpu...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n",
      "No framework detected, defaulting to jax\n",
      "[INFO] Detected framework: jax\n"
     ]
    }
   ],
   "source": [
    "import dyson\n",
    "\n",
    "compiled_func = dyson.run(titanic_etl, target_device=hardware['hardware_type'])\n",
    "\n",
    "# Run the ETL function\n",
    "processed_data = compiled_func(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_Rare</th>\n",
       "      <th>Deck_A</th>\n",
       "      <th>Deck_B</th>\n",
       "      <th>Deck_C</th>\n",
       "      <th>Deck_D</th>\n",
       "      <th>Deck_E</th>\n",
       "      <th>Deck_F</th>\n",
       "      <th>Deck_G</th>\n",
       "      <th>Deck_T</th>\n",
       "      <th>Deck_U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare  \\\n",
       "0            1         0       3    0  22.0      1      0   7.2500   \n",
       "1            2         1       1    1  38.0      1      0  71.2833   \n",
       "2            3         1       3    1  26.0      0      0   7.9250   \n",
       "3            4         1       1    1  35.0      1      0  53.1000   \n",
       "4            5         0       3    0  35.0      0      0   8.0500   \n",
       "\n",
       "   FamilySize  IsAlone  ...  Title_Rare  Deck_A  Deck_B  Deck_C  Deck_D  \\\n",
       "0           2        0  ...       False   False   False   False   False   \n",
       "1           2        0  ...       False   False   False    True   False   \n",
       "2           1        1  ...       False   False   False   False   False   \n",
       "3           2        0  ...       False   False   False    True   False   \n",
       "4           1        1  ...       False   False   False   False   False   \n",
       "\n",
       "   Deck_E  Deck_F  Deck_G  Deck_T  Deck_U  \n",
       "0   False   False   False   False    True  \n",
       "1   False   False   False   False   False  \n",
       "2   False   False   False   False    True  \n",
       "3   False   False   False   False   False  \n",
       "4   False   False   False   False    True  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the processed data\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for t4...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n",
      "No framework detected, defaulting to jax\n",
      "[INFO] Detected framework: jax\n"
     ]
    }
   ],
   "source": [
    "import dyson\n",
    "\n",
    "compiled_func = dyson.run(titanic_etl, target_device='t4')\n",
    "\n",
    "# Run the ETL function\n",
    "processed_data = compiled_func(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_Rare</th>\n",
       "      <th>Deck_A</th>\n",
       "      <th>Deck_B</th>\n",
       "      <th>Deck_C</th>\n",
       "      <th>Deck_D</th>\n",
       "      <th>Deck_E</th>\n",
       "      <th>Deck_F</th>\n",
       "      <th>Deck_G</th>\n",
       "      <th>Deck_T</th>\n",
       "      <th>Deck_U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare  \\\n",
       "0            1         0       3    0  22.0      1      0   7.2500   \n",
       "1            2         1       1    1  38.0      1      0  71.2833   \n",
       "2            3         1       3    1  26.0      0      0   7.9250   \n",
       "3            4         1       1    1  35.0      1      0  53.1000   \n",
       "4            5         0       3    0  35.0      0      0   8.0500   \n",
       "\n",
       "   FamilySize  IsAlone  ...  Title_Rare  Deck_A  Deck_B  Deck_C  Deck_D  \\\n",
       "0           2        0  ...       False   False   False   False   False   \n",
       "1           2        0  ...       False   False   False    True   False   \n",
       "2           1        1  ...       False   False   False   False   False   \n",
       "3           2        0  ...       False   False   False    True   False   \n",
       "4           1        1  ...       False   False   False   False   False   \n",
       "\n",
       "   Deck_E  Deck_F  Deck_G  Deck_T  Deck_U  \n",
       "0   False   False   False   False    True  \n",
       "1   False   False   False   False   False  \n",
       "2   False   False   False   False    True  \n",
       "3   False   False   False   False   False  \n",
       "4   False   False   False   False    True  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Results\n",
    "\n",
    "| Hardware    | Execution Time | Hourly Cost                    |\n",
    "|-------------|----------------|--------------------------------|\n",
    "| **GPU T4**  | 5.3 seconds    | $1.00 (Standard 8 vCPU)        |\n",
    "| **C4A CPU** | 5.3 seconds    | $0.21                          |\n",
    "\n",
    "### Observations:\n",
    "- **Performance:**  \n",
    "  - Both C4A CPU and T4 GPU complete the task in **5.3 seconds**.\n",
    "  - The function successfully demonstrates remote data access capabilities, fetching an image from GitHub and processing it.\n",
    "  \n",
    "- **Cost Efficiency:**  \n",
    "  - The C4A CPU is significantly more cost-effective at **$0.21 per hour** compared to **$1.00 per hour** for the T4 GPU instance.\n",
    "\n",
    "## Cost per Execution Analysis\n",
    "\n",
    "| Hardware    | Execution Time (sec) | Hourly Cost (USD) | Cost per Execution (USD) | \n",
    "|-------------|----------------------|-------------------|--------------------------|\n",
    "| **GPU T4**  | 5.3                  | $1.00             | $0.00147                 |\n",
    "| **C4A CPU** | 5.3                  | $0.21             | $0.00031                 |\n",
    "\n",
    "### Cost Savings Analysis\n",
    "\n",
    "- **Cost Savings per Execution:** $0.00116 ($0.00147 - $0.00031)\n",
    "- **Percentage Cost Savings:** 78.9%\n",
    "- **Cost Ratio:** C4A CPU is 4.7x more cost-efficient per execution than GPU T4\n",
    "\n",
    "## Key Highlights\n",
    "\n",
    "1. **Remote Data Access:** The function successfully demonstrates Dyson's ability to access and process remote data (images) from external URLs.\n",
    "\n",
    "2. **Performance Equality:** The C4A CPU and T4 GPU perform identically for this specific workload involving remote data access and basic image processing.\n",
    "\n",
    "3. **Cost Efficiency:** C4A CPU offers substantial cost savings (78.9% lower cost per execution) compared to T4 GPU.\n",
    "\n",
    "## Recommendation\n",
    "\n",
    "For workloads involving remote data access and basic image processing:\n",
    "- **C4A CPU** is strongly recommended for cost efficiency while maintaining the same performance as T4 GPU\n",
    "- This configuration is ideal for production deployments where cost optimization is important without sacrificing speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With dyson FFI you can run cpp code also in log mode and eager mode here the example how you can route a cpp code with dyson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_code = \"\"\"\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "#include <string.h>\n",
    "#include <float.h>\n",
    "\n",
    "extern \"C\" {\n",
    "    // Vector operations that take arrays as input and return arrays\n",
    "    \n",
    "    // Apply sigmoid activation to an entire array\n",
    "    void sigmoid_array(float* input, float* output, int size) {\n",
    "        for (int i = 0; i < size; i++) {\n",
    "            output[i] = 1.0f / (1.0f + exp(-input[i]));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Batch normalization (simplified version)\n",
    "    void batch_norm(float* input, float* output, int size, float epsilon) {\n",
    "        // Calculate mean\n",
    "        float mean = 0.0f;\n",
    "        for (int i = 0; i < size; i++) {\n",
    "            mean += input[i];\n",
    "        }\n",
    "        mean /= size;\n",
    "        \n",
    "        // Calculate variance\n",
    "        float variance = 0.0f;\n",
    "        for (int i = 0; i < size; i++) {\n",
    "            float diff = input[i] - mean;\n",
    "            variance += diff * diff;\n",
    "        }\n",
    "        variance /= size;\n",
    "        \n",
    "        // Normalize\n",
    "        for (int i = 0; i < size; i++) {\n",
    "            output[i] = (input[i] - mean) / sqrtf(variance + epsilon);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Element-wise multiplication of two arrays\n",
    "    void hadamard_product(float* a, float* b, float* output, int size) {\n",
    "        for (int i = 0; i < size; i++) {\n",
    "            output[i] = a[i] * b[i];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Convolution operation (1D)\n",
    "    void conv1d(float* input, float* kernel, float* output, int input_size, int kernel_size) {\n",
    "        int output_size = input_size - kernel_size + 1;\n",
    "        \n",
    "        for (int i = 0; i < output_size; i++) {\n",
    "            output[i] = 0.0f;\n",
    "            for (int j = 0; j < kernel_size; j++) {\n",
    "                output[i] += input[i + j] * kernel[j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Feature transformation - combines multiple operations\n",
    "    // This function demonstrates a more complex pipeline that:\n",
    "    // 1. Applies convolution\n",
    "    // 2. Normalizes the result\n",
    "    // 3. Applies sigmoid activation\n",
    "    float* feature_transform(float* input, float* kernel, \n",
    "                          int input_size, int kernel_size, float epsilon) {\n",
    "        \n",
    "        int output_size = input_size - kernel_size + 1;\n",
    "        \n",
    "        // Allocate temporary buffers\n",
    "        float* conv_output = (float*)malloc(output_size * sizeof(float));\n",
    "        float* norm_output = (float*)malloc(output_size * sizeof(float));\n",
    "        float* output = (float*)malloc(output_size * sizeof(float));\n",
    "        \n",
    "        // Apply convolution\n",
    "        conv1d(input, kernel, conv_output, input_size, kernel_size);\n",
    "        \n",
    "        // Apply batch normalization\n",
    "        batch_norm(conv_output, norm_output, output_size, epsilon);\n",
    "        \n",
    "        // Apply sigmoid activation\n",
    "        sigmoid_array(norm_output, output, output_size);\n",
    "        \n",
    "        // Free temporary buffers\n",
    "        free(conv_output);\n",
    "        free(norm_output);\n",
    "        return output;\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama-7b response: {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '  As a hardware optimization expert, I recommend using a CPU-first approach for this workload. Based on the analysis, there are several reasons why CPU processing may be sufficient:\\n\\n1. Low operation count: The workload has a low operation count, which suggests that CPU processing may be sufficient for most tasks.\\n2. Small to medium batch size: The batch size is small to medium, which means that CPU processing can handle most of the workload without requiring a high-performance GPU.\\n3. Low computational intensity: The workload has a low computational intensity, which means that GPU acceleration may not be necessary for most tasks.\\n4. Energy efficiency: Energy efficiency is a critical factor, and using a CPU-first approach can help reduce energy consumption.\\n\\nBased on these factors, I recommend using a CPU-first approach for this workload. If the workload requires more computational power, a GPU can be added later.\\n\\nHere are some justifications for the CPU-first approach:\\n\\n1. Element-wise operations: The workload involves element-wise operations, which can be efficiently handled by CPUs.\\n2. Low parallelism: The workload has low parallelism, which means that GPU acceleration may not be necessary for most tasks.\\n3. Sequential memory access: The workload has sequential memory access patterns, which can be efficiently handled by CPUs.\\n\\nBased on these justifications, I estimate that the CPU-first approach can provide an approximate speedup of 3-5x compared to using a high-performance GPU. The cost efficiency rating would be around 7-9, and the energy efficiency rating would be around 9-10.\\n\\nTherefore, I recommend using a CPU-first approach for this workload, and considering a GPU addition later if necessary.', 'role': 'assistant', 'tool_calls': []}}], 'created': 1742550081, 'id': 'cmpl-5ddf7903cae142edaca921fa57ea53c9', 'model': 'Llama-2-7b-chat', 'object': 'chat.completion', 'usage': {'completion_tokens': 385, 'prompt_tokens': 1788, 'total_tokens': 2173}}\n",
      "Model AI21 response: {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': ' {\\n  \"hardware_type\": \"c4cpu\",\\n  \"spec\": \"CPU-first approach, medium computational intensity, matrix operations detected\",\\n  \"estimated_speedup\": \"N/A (baseline comparison)\",\\n  \"cost_efficiency_rating\": 9,\\n  \"energy_efficiency_rating\": 8,\\n  \"confidence\": \"high\"\\n}\\n\\n**Justification:**\\n\\n1. **CPU-First Approach:** The workload exhibits medium computational intensity and involves matrix operations. Given the small matrix dimensions and low operation count, a CPU is likely sufficient and more cost-effective.\\n\\n2. **Operation Types:** Linear algebra and transformation operations are well-suited for CPU processing, especially with the provided frameworks (CPP, NumPy).\\n\\n3. **Cost and Energy Efficiency:** CPUs offer high cost and energy efficiency compared to GPUs, making them a suitable choice for this balanced optimization mode.\\n\\n4. **Confidence:** The analysis is based on the provided task details and hardware configurations, leading to a high confidence level in the recommendation.\\n\\n\\n\\n**Recommendation:**\\n\\nUtilize a compute-optimized CPU (c4cpu) for this workload. This choice prioritizes cost and energy efficiency while providing sufficient processing power for the medium computational intensity and matrix operations involved.\\n\\n\\n\\n\\n', 'role': 'assistant', 'tool_calls': None}}], 'created': 1742550092, 'id': 'chat-bf23dc07f033458b99463587ead93ebb', 'model': 'jamba-1.5-mini', 'object': 'chat.completion', 'usage': {'completion_tokens': 289, 'prompt_tokens': 1656, 'total_tokens': 1945}}\n",
      "Model Llama-3.1-8B response: {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Based on the task analysis, I recommend the following hardware configuration:\\n\\n```json\\n{\\n    \"hardware_type\": \"c4cpu\",\\n    \"spec\": \"Low operation count with small matrices, strongly suggesting CPU would be sufficient and more cost-effective. Medium computational intensity detected, but CPU processing would still be sufficient.\",\\n    \"estimated_speedup\": \"1.2-1.5x\",\\n    \"cost_efficiency_rating\": \"9/10\",\\n    \"energy_efficiency_rating\": \"8/10\",\\n    \"confidence\": \"high\"\\n}\\n```\\n\\nJustification:\\n\\n1.  **CPU-FIRST APPROACH**: The operations are primarily element-wise (linear_algebra, transformation_ops), and the total matrix elements are likely under 1M. The task is also likely to be embarrassingly parallel across CPU cores, making CPU processing a suitable choice.\\n2.  **Medium computational intensity**: While the computational intensity is medium, the operation count is low, and the matrix dimensions are not large. This suggests that CPU processing would still be sufficient and more cost-effective.\\n3.  **Energy efficiency**: Energy efficiency is critical, and the compute-optimized CPU (c4cpu) offers excellent energy efficiency ratings.\\n4.  **Cost efficiency**: The compute-optimized CPU (c4cpu) has a high cost efficiency rating, making it a cost-effective choice.\\n\\nThe compute-optimized CPU (c4cpu) is the most suitable choice for this workload, given its energy efficiency, cost efficiency, and ability to handle CPU-intensive computations. The estimated speedup is around 1.2-1.5x compared to the baseline general-purpose CPU (n4cpu), and the confidence level is high.', 'role': 'assistant', 'tool_calls': None}}], 'created': 1742550095, 'id': 'cmpl-cf7fd147-9d77-432b-aed5-7f6087f3c078', 'model': 'Meta-Llama-3.1-8B-Instruct', 'object': 'chat.completion', 'usage': {'completion_tokens': 344, 'prompt_tokens': 1464, 'total_tokens': 1808}}\n",
      "Error calling Llama-2-13b: 400, {\"error\":{\"code\":\"Bad Request\",\"message\":\"{\\\"object\\\":\\\"error\\\",\\\"message\\\":\\\"Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating\\\",\\\"type\\\":\\\"BadRequestError\\\",\\\"param\\\":null,\\\"code\\\":400}\",\"status\":400}}\n",
      "Judge decision: {'hardware_type': 'c4cpu', 'spec': 'The workload exhibits medium computational intensity and involves matrix operations. Given the small matrix dimensions and low operation count, a CPU is likely sufficient and more cost-effective. CPUs offer high cost and energy efficiency compared to GPUs, making them a suitable choice for this balanced optimization mode.', 'confidence': 'high', 'cost_efficiency_score': 9, 'energy_efficiency_score': 8, 'performance_score': 7, 'alternative_option': 'c4acpu'}\n",
      "Routed to: {'hardware_type': 'c4cpu', 'spec': 'The workload exhibits medium computational intensity and involves matrix operations. Given the small matrix dimensions and low operation count, a CPU is likely sufficient and more cost-effective. CPUs offer high cost and energy efficiency compared to GPUs, making them a suitable choice for this balanced optimization mode.', 'confidence': 'high', 'cost_efficiency_score': 9, 'energy_efficiency_score': 8, 'performance_score': 7, 'alternative_option': 'c4acpu'}\n"
     ]
    }
   ],
   "source": [
    "hardware = router.route_hardware(cpp_code, mode=\"balanced\")\n",
    "print(f\"Routed to: {hardware}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c4cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardware['hardware_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo the more complex feature_transform function\n",
    "import numpy as np\n",
    "input_size = 8\n",
    "kernel_size = 3\n",
    "output_size = input_size - kernel_size + 1\n",
    "\n",
    "# Create input, kernel, and output arrays\n",
    "input_array = np.array([0.5, -0.3, 0.7, -0.2, 0.1, 0.8, -0.5, 0.4], dtype=np.float32)\n",
    "kernel = np.array([0.1, 0.2, 0.3], dtype=np.float32)\n",
    "output_array = np.zeros(output_size, dtype=np.float32)\n",
    "epsilon = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized CppFunction with function name: feature_transform, return_type: float_array, return_size: 6\n"
     ]
    }
   ],
   "source": [
    "feature_transform_func = dyson.CppFunction(\n",
    "    cpp_code=cpp_code,\n",
    "    function_name=\"feature_transform\",\n",
    "    return_type=\"float_array\",\n",
    "    return_size=output_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Compiling function for c4cpu...\n",
      "Compiling function  Done!\n",
      "[STATUS] Instance status: RUNNING\n",
      "[INFO] C++ function detected: feature_transform\n"
     ]
    }
   ],
   "source": [
    "# Compile the feature_transform function\n",
    "compiled_transform = dyson.run(feature_transform_func, target_device=hardware['hardware_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Executing C++ function 'feature_transform' on c4cpu\n",
      "\n",
      "Feature Transform Demo:\n",
      "Input array: [ 0.5 -0.3  0.7 -0.2  0.1  0.8 -0.5  0.4]\n",
      "Kernel: [0.1 0.2 0.3]\n",
      "Transformed output: [0.74854153 0.3183128  0.34568882 0.82988584 0.24378975 0.46404356]\n"
     ]
    }
   ],
   "source": [
    "# Run the transformation\n",
    "res = compiled_transform(input_array, kernel, input_size, kernel_size, epsilon)\n",
    "\n",
    "print(\"\\nFeature Transform Demo:\")\n",
    "print(\"Input array:\", input_array)\n",
    "print(\"Kernel:\", kernel)\n",
    "print(\"Transformed output:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
